---
title: "PPO Control with the Standard Open ARM101"
excerpt: "Implementation of the Proximal Policy Optimization (PPO) algorithm, tested with the Standard Open ARM101 in IsaacLab and deployed in the real world"
collection: portfolio
---
Using my implementation of PPO, I wanted to train a model within the IsaacLab simulator, and deploy it on a real-world robot, the SO-ARM101. Currently, I have trained two policies for the robot, and I am currently in the process of deploying the model to the real world. I am awaiting my robot arm to arrive, which I will then need to assemble and test. I am currently working on the necessary code development to transition the model from IsaacLab to the real-world. 

My implementation of PPO that I am using can be found [here](https://github.com/ryan-donald/PPO_IsaacLab).

My trained models are shown below, both the final visual performance, and the average reward during training.

<p float="left">
    <img src='/images/training_plot_episodes_Isaac-Lift-Cube-SO-ARM101-v0.svg' onerror this.src='/images/training_plot_episodes_Isaac-Lift-Cube-SO-ARM101-v0.png' width='49%'> <img src='/images/so101_lift.gif' width="49%">  
</p>
<p float="left">
    <img src='/images/training_plot_episodes_Isaac-Reach-SO-ARM101-v0.svg' onerror this.src='/images/training_plot_episodes_Isaac-Reach-SO-ARM101-v0.png' width='49%'> <img src='/images/so101_reach.gif' width="49%"> 
</p>